{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# load MNIST DIGITS\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# normalize the train and test sets\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# one-hot encode the labels\n",
    "y_train = tf.one_hot(y_train, 10)\n",
    "y_test = tf.one_hot(y_test, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "width = 28  # width of the image in pixels\n",
    "height = 28  # height of the image in pixels\n",
    "flat = width * height  # number of pixels in one image\n",
    "class_output = 10  # number of possible classifications for the problem"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# reshape train and test data\n",
    "# the input image is 28 pixels by 28 pixels, 1 channel (grayscale). in this case, the first dimension is the batch number of the image\n",
    "# it can be of any size (so we set it to -1). the second and third dimensions are width and height, and the last one is the image channels.\n",
    "\n",
    "x_image_train = tf.reshape(x_train, [-1, 28, 28, 1])\n",
    "x_image_train = tf.cast(x_image_train, 'float32')\n",
    "\n",
    "x_image_test = tf.reshape(x_test, [-1, 28, 28, 1])\n",
    "x_image_test = tf.cast(x_image_test, 'float32')\n",
    "\n",
    "# creating new dataset with reshaped inputs\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_image_train, y_train)).batch(50)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_image_test, y_test)).batch(50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Weights Biases and Convolutional Layer Definitions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# we define a kernel here. the size of the filter/kernel is 5x5; input channels is 1 (grayscale); and we need 32 different\n",
    "# feature maps (here, 32 feature maps means 32 different filters are applied on each image, so the output of the convolution\n",
    "# layer would be 28x28x32). in this step, we create a filter/kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "\n",
    "W_conv1 = tf.Variable(tf.random.truncated_normal([5, 5, 1, 32], stddev=0.1, seed=0))\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape=[32]))  # need 32 biases for 32 outputs\n",
    "\n",
    "\n",
    "# 1st convolutional layer\n",
    "def conv_layer_1(x):\n",
    "    # 1st convolution\n",
    "    x = tf.nn.conv2d(x, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1\n",
    "    # relu\n",
    "    x = tf.nn.relu(x)\n",
    "    # max_pool\n",
    "    x = tf.nn.max_pool(\n",
    "        x,\n",
    "        ksize=[1, 2, 2, 1],\n",
    "        strides=[1, 2, 2, 1],\n",
    "        padding=\"SAME\"\n",
    "    )\n",
    "    return x\n",
    "\n",
    "\n",
    "W_conv2 = tf.Variable(tf.random.truncated_normal([5, 5, 32, 64], stddev=0.1, seed=1))\n",
    "b_conv2 = tf.Variable(tf.constant(0.1, shape=[64]))  # need 64 biases for 64 outputs\n",
    "\n",
    "\n",
    "# 2nd convolutional layer\n",
    "def conv_layer_2(x):\n",
    "    # 1st convolution\n",
    "    x = tf.nn.conv2d(x, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2\n",
    "    # relu\n",
    "    x = tf.nn.relu(x)\n",
    "    # max_pool\n",
    "    x = tf.nn.max_pool(\n",
    "        x,\n",
    "        ksize=[1, 2, 2, 1],\n",
    "        strides=[1, 2, 2, 1],\n",
    "        padding=\"SAME\"\n",
    "    )\n",
    "    return x\n",
    "\n",
    "\n",
    "# flatten for outputs\n",
    "def flatten_conv_volume(x):\n",
    "    x = tf.reshape(x, [-1, 7 * 7 * 64])\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fully Connected Layer Definitions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# fc weights and biases\n",
    "W_fc1 = tf.Variable(tf.random.truncated_normal([7 * 7 * 64, 1024], stddev=0.1, seed=2))\n",
    "b_fc1 = tf.Variable(tf.constant(0.1, shape=[1024]))  # need 1024 biases for 1024 outputs\n",
    "\n",
    "\n",
    "def fc_1(x):\n",
    "    x = tf.matmul(x, W_fc1) + b_fc1\n",
    "    x = tf.nn.relu(x)\n",
    "    # add some dropout to avoid overfitting\n",
    "    # set a rate of 0.3 (this was chosen somewhat arbitrarily)\n",
    "    x = tf.nn.dropout(x, 0.3)\n",
    "    return x\n",
    "\n",
    "\n",
    "W_fc2 = tf.Variable(tf.random.truncated_normal([1024, 10], stddev=0.1, seed=2))  # 1024 neurons\n",
    "b_fc2 = tf.Variable(tf.constant(0.1, shape=[10]))  # 10 possibilities for digits [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "\n",
    "def fc_2(x):\n",
    "    x = tf.matmul(x, W_fc2) + b_fc2\n",
    "    x = tf.nn.relu(x)\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Full Network with Softmax Output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def y_hat_cnn(x):\n",
    "    x = conv_layer_1(x)\n",
    "    x = conv_layer_2(x)\n",
    "    x = flatten_conv_volume(x)\n",
    "    x = fc_1(x)\n",
    "    x = fc_2(x)\n",
    "    y_hat = tf.nn.softmax(x)\n",
    "    return y_hat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loss Function and Optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y_pred):\n",
    "    return -tf.reduce_sum(y_hat * tf.math.log(y_pred + 1.e-10))\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# variables that gradient tape has to watch\n",
    "variables = [W_conv1, b_conv1, W_conv2, b_conv2, W_fc1, b_fc1, W_fc2, b_fc2]\n",
    "\n",
    "\n",
    "# single training step (possibly over a batch)\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_hat = y_hat_cnn(x)\n",
    "        current_loss = cross_entropy(y, y_hat)\n",
    "        grads = tape.gradient(current_loss, variables)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        return current_loss.numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n",
      "Starting epoch: 1\n",
      "############\n",
      "Batch: 50, loss: 107.37092590332031, accuracy: 0.07999999821186066\n",
      "Batch: 100, loss: 92.30964660644531, accuracy: 0.2800000011920929\n",
      "Batch: 150, loss: 97.21812438964844, accuracy: 0.3400000035762787\n",
      "Batch: 200, loss: 83.60059356689453, accuracy: 0.4000000059604645\n",
      "Batch: 250, loss: 74.81949615478516, accuracy: 0.47999998927116394\n",
      "Batch: 300, loss: 63.296043395996094, accuracy: 0.5400000214576721\n",
      "Batch: 350, loss: 48.380313873291016, accuracy: 0.6399999856948853\n",
      "Batch: 400, loss: 54.9945182800293, accuracy: 0.6600000262260437\n",
      "Batch: 450, loss: 49.34773254394531, accuracy: 0.699999988079071\n",
      "Batch: 500, loss: 32.57860565185547, accuracy: 0.8600000143051147\n",
      "Batch: 550, loss: 12.160505294799805, accuracy: 0.9599999785423279\n",
      "Batch: 600, loss: 17.0562744140625, accuracy: 0.8999999761581421\n",
      "Batch: 650, loss: 17.437807083129883, accuracy: 0.8999999761581421\n",
      "Batch: 700, loss: 6.042573928833008, accuracy: 0.9599999785423279\n",
      "Batch: 750, loss: 24.068450927734375, accuracy: 0.9200000166893005\n",
      "Batch: 800, loss: 7.615208625793457, accuracy: 0.9399999976158142\n",
      "Batch: 850, loss: 16.072385787963867, accuracy: 0.8999999761581421\n",
      "Batch: 900, loss: 13.482266426086426, accuracy: 0.9399999976158142\n",
      "Batch: 950, loss: 11.71091365814209, accuracy: 0.9399999976158142\n",
      "Batch: 1000, loss: 9.500031471252441, accuracy: 0.9399999976158142\n",
      "Batch: 1050, loss: 5.883028507232666, accuracy: 0.9800000190734863\n",
      "Batch: 1100, loss: 13.291439056396484, accuracy: 0.9599999785423279\n",
      "Batch: 1150, loss: 8.065741539001465, accuracy: 0.9399999976158142\n",
      "Batch: 1200, loss: 2.1181859970092773, accuracy: 1.0\n",
      "\n",
      "############\n",
      "End of epoch: 1\n",
      "\tloss: 1705.324462890625\n",
      "\taccuracy: 0.9501000046730042\n",
      "############\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "loss_values = []\n",
    "accuracies = []\n",
    "epochs = 1\n",
    "\n",
    "for i in range(epochs):\n",
    "    j = 0\n",
    "    # each batch has 50 examples\n",
    "    print(f\"############\\nStarting epoch: {i + 1}\\n############\")\n",
    "    for x_train_batch, y_train_batch in train_ds:\n",
    "        j += 1\n",
    "        current_loss = train_step(x_train_batch, y_train_batch)\n",
    "        if j % 50 == 0:  #reporting intermittent batch statistics\n",
    "            correct_prediction = tf.equal(\n",
    "                tf.argmax(y_hat_cnn(x_train_batch), axis=1),\n",
    "                tf.argmax(y_train_batch, axis=1)\n",
    "            )\n",
    "            #  accuracy\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)).numpy()\n",
    "            print(f\"Batch: {j}, loss: {current_loss}, accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "    current_loss = cross_entropy(y_train[:10000], y_hat_cnn(x_image_train[:10000, :, :, :])).numpy()\n",
    "    loss_values.append(current_loss)\n",
    "    print()\n",
    "    correct_prediction = tf.equal(\n",
    "        tf.argmax(y_hat_cnn(x_image_train[:10000, :, :, :]), axis=1),\n",
    "        tf.argmax(y_train[:10000], axis=1)\n",
    "    )\n",
    "    #  accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)).numpy()\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"############\\nEnd of epoch: {i + 1}\\n\\tloss: {current_loss}\\n\\taccuracy: {accuracy}\\n############\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBatch 100, accuracy: 0.8999999761581421\n",
      "\tBatch 200, accuracy: 0.9599999785423279\n",
      "\n",
      "Accuracy of entire set: 0.9519000053405762\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on validation set\n",
    "j = 0\n",
    "acc = []\n",
    "# evaluate accuracy by batch and average...reporting every 100th batch\n",
    "for x_train_batch, y_train_batch in test_ds:\n",
    "    j += 1\n",
    "    correct_prediction = tf.equal(\n",
    "        tf.argmax(y_hat_cnn(x_train_batch), axis=1),\n",
    "        tf.argmax(y_train_batch, axis=1)\n",
    "    )\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)).numpy()\n",
    "    acc.append(accuracy)\n",
    "    if j % 100 == 0:\n",
    "        print(f\"\\tBatch {j}, accuracy: {accuracy}\")\n",
    "\n",
    "print(f\"\\nAccuracy of entire set: {np.mean(acc)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}